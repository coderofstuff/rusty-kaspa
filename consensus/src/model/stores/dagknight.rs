use std::{cell::RefCell, collections::HashMap, sync::Arc};

use kaspa_consensus_core::KType;
use kaspa_database::{
    prelude::{DbKey, StoreError},
    registry::DatabaseStorePrefixes,
};
use kaspa_hashes::Hash;

use crate::model::stores::ghostdag::GhostdagData;
use kaspa_database::prelude::{BatchDbWriter, CachePolicy, CachedDbAccess, DB};
use rocksdb::WriteBatch;

pub struct MemoryDagknightStore {
    dk_map: RefCell<HashMap<DagknightKey, Arc<GhostdagData>>>,
}

pub trait DagknightStoreReader {
    fn get_selected_parent(&self, dk_key: DagknightKey) -> Result<Hash, StoreError>;
    fn get_data(&self, dk_key: DagknightKey) -> Result<Arc<GhostdagData>, StoreError>;
}

#[derive(Clone)]
pub struct DagknightKey {
    pub pov_hash: Hash,
    pub root_hash: Hash,
    pub k: KType,
    pub free_search: bool,
    // Precomputed bytes in order: root_hash || k(u16 BE) || pov_hash || free_search
    bytes: [u8; kaspa_hashes::HASH_SIZE * 2 + 3],
}

impl DagknightKey {
    pub fn new(root_hash: Hash, pov_hash: Hash, k: KType, free_search: bool) -> Self {
        // Layout must match DB-level expectations where `k` is encoded as a u16
        // (two bytes). Allocate enough space: root_hash + k(2) + pov_hash + free_search(1).
        let mut bytes = [0u8; kaspa_hashes::HASH_SIZE * 2 + 3];
        let hash_size = kaspa_hashes::HASH_SIZE;
        bytes[..hash_size].copy_from_slice(root_hash.as_ref());

        // Encode k as big-endian u16 to match other code paths that construct
        // DB keys using two bytes for k.
        let k_be = k.to_be_bytes();
        bytes[hash_size] = k_be[0];
        bytes[hash_size + 1] = k_be[1];

        bytes[(hash_size + 2)..(hash_size + 2 + hash_size)].copy_from_slice(pov_hash.as_ref());
        bytes[(2 * hash_size) + 2] = if free_search { 1 } else { 0 };

        Self { pov_hash, root_hash, k, free_search, bytes }
    }
}

impl ToString for DagknightKey {
    fn to_string(&self) -> String {
        format!("{:?}", &self.bytes)
    }
}

impl AsRef<[u8]> for DagknightKey {
    fn as_ref(&self) -> &[u8] {
        &self.bytes
    }
}

impl Eq for DagknightKey {}

impl std::hash::Hash for DagknightKey {
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        // Hash based on the logical key fields
        self.root_hash.hash(state);
        self.k.hash(state);
        self.pov_hash.hash(state);
        self.free_search.hash(state);
    }
}

impl PartialEq for DagknightKey {
    fn eq(&self, other: &Self) -> bool {
        self.pov_hash == other.pov_hash
            && self.k == other.k
            && self.root_hash == other.root_hash
            && self.free_search == other.free_search
    }
}

pub trait DagknightStore {
    fn insert(&self, key: DagknightKey, dk_data: Arc<GhostdagData>) -> Result<(), StoreError>;
    fn delete(&self, key: DagknightKey) -> Result<(), StoreError>;
    fn delete_rooted_range(&self, batch: &mut WriteBatch, hash: Hash) -> Result<u32, StoreError>;
}

impl MemoryDagknightStore {
    pub fn new(dk_map: RefCell<HashMap<DagknightKey, Arc<GhostdagData>>>) -> Self {
        Self { dk_map }
    }
}

impl DagknightStoreReader for MemoryDagknightStore {
    fn get_selected_parent(&self, dk_key: DagknightKey) -> Result<Hash, StoreError> {
        Ok(self.get_data(dk_key)?.selected_parent)
    }

    fn get_data(&self, key: DagknightKey) -> Result<Arc<GhostdagData>, StoreError> {
        if let Some(pov_block_dk_data) = self.dk_map.borrow().get(&key) {
            Ok(pov_block_dk_data.clone())
        } else {
            Err(StoreError::KeyNotFound(DbKey::new(DatabaseStorePrefixes::DagKnight.as_ref(), key)))
        }
    }
}

impl DagknightStore for MemoryDagknightStore {
    fn insert(&self, key: DagknightKey, dk_data: Arc<GhostdagData>) -> Result<(), StoreError> {
        self.dk_map.borrow_mut().insert(key, dk_data);

        Ok(())
    }

    fn delete(&self, key: DagknightKey) -> Result<(), StoreError> {
        self.dk_map.borrow_mut().remove(&key);

        Ok(())
    }

    fn delete_rooted_range(&self, _batch: &mut WriteBatch, _hash: Hash) -> Result<u32, StoreError> {
        unimplemented!()
    }
}

/// A DB + cache implementation of `DagknightStore` trait, with concurrency support.
#[derive(Clone)]
pub struct DbDagknightStore {
    db: Arc<DB>,
    access: CachedDbAccess<DagknightKey, Arc<GhostdagData>>,
}

impl DbDagknightStore {
    pub fn new(db: Arc<DB>, cache_policy: CachePolicy) -> Self {
        let prefix = DatabaseStorePrefixes::DagKnight.as_ref().to_vec();
        Self { db: Arc::clone(&db), access: CachedDbAccess::new(db, cache_policy, prefix) }
    }

    pub fn insert_batch(&self, batch: &mut WriteBatch, key: DagknightKey, data: Arc<GhostdagData>) -> Result<(), StoreError> {
        if self.access.has(key.clone())? {
            return Err(StoreError::KeyAlreadyExists(key.to_string()));
        }
        self.access.write(BatchDbWriter::new(batch), key, data)?;
        Ok(())
    }

    pub fn delete_batch(&self, batch: &mut WriteBatch, key: DagknightKey) -> Result<(), StoreError> {
        self.access.delete(BatchDbWriter::new(batch), key)
    }
}

impl DagknightStoreReader for DbDagknightStore {
    fn get_selected_parent(&self, dk_key: DagknightKey) -> Result<Hash, StoreError> {
        Ok(self.get_data(dk_key)?.selected_parent)
    }

    fn get_data(&self, dk_key: DagknightKey) -> Result<Arc<GhostdagData>, StoreError> {
        self.access.read(dk_key)
    }
}

impl DagknightStore for DbDagknightStore {
    fn insert(&self, key: DagknightKey, dk_data: Arc<GhostdagData>) -> Result<(), StoreError> {
        if self.access.has(key.clone())? {
            return Err(StoreError::KeyAlreadyExists(key.to_string()));
        }
        let mut batch = WriteBatch::default();
        self.access.write(BatchDbWriter::new(&mut batch), key, dk_data)?;
        self.db.write(batch)?;
        Ok(())
    }

    fn delete(&self, key: DagknightKey) -> Result<(), StoreError> {
        let mut batch = WriteBatch::default();
        self.access.delete(BatchDbWriter::new(&mut batch), key)?;
        self.db.write(batch)?;
        Ok(())
    }

    fn delete_rooted_range(&self, batch: &mut WriteBatch, hash: Hash) -> Result<u32, StoreError> {
        // delete records that have a prefix rooted at this DK store key + hash
        let root_bytes_prefix = {
            let mut bytes = Vec::with_capacity(kaspa_hashes::HASH_SIZE + 1);
            bytes.extend(DatabaseStorePrefixes::DagKnight.as_ref());
            bytes.extend_from_slice(hash.as_ref());
            bytes
        };
        let start_conflict_genesis_bytes = {
            let mut bytes = Vec::with_capacity(kaspa_hashes::HASH_SIZE + 2);
            bytes.extend_from_slice(&root_bytes_prefix);
            bytes.push(0); // k = 0 u16 first byte
            bytes.push(0); // k = 0 u16 second byte
            bytes
        };
        let end_conflict_genesis_bytes = {
            let mut bytes = Vec::with_capacity(kaspa_hashes::HASH_SIZE + 2);
            bytes.extend_from_slice(&root_bytes_prefix);
            // TODO[DK]: This range check misses entries where k = u16::MAX. However, we don't expect k to reach that value anyway
            // in practice so we don't expect records to exist here as well. In the DK implementation, k may be clamped to max out
            // lower than k = u16::MAX
            bytes.push(0xFF); // k = 0xFFFF u16 first byte
            bytes.push(0xFF); // k = 0xFFFF u16 second byte
            bytes
        };
        // TODO[DK]: count keys in range. Possibly would be removed.
        let mut count = 0;
        let mut iterator = self.db.raw_iterator();
        iterator.seek(&start_conflict_genesis_bytes);
        while iterator.valid() {
            let key = iterator.key();
            if key.unwrap() >= end_conflict_genesis_bytes.as_slice() {
                break;
            }
            count += 1;
            iterator.next();
        }
        // Perform the range delete
        batch.delete_range(start_conflict_genesis_bytes, end_conflict_genesis_bytes);
        Ok(count)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_dagknight_key_encodes_k() {
        use crate::model::stores::dagknight::DagknightKey;
        use kaspa_hashes::Hash;

        let root: Hash = 0xAA_u64.into();
        let pov: Hash = 0xBB_u64.into();
        let k1: KType = 0x0001;
        let k2: KType = 0x0101; // 2nd byte is the same as above

        let key1 = DagknightKey::new(root, pov, k1, false);
        let key2 = DagknightKey::new(root, pov, k2, false);

        // The DB key bytes must differ when k differs. This captures the previous
        // bug where `k` was encoded incorrectly and keys collided across k values.
        println!("key1 bytes: {:?}", key1.as_ref());
        println!("key2 bytes: {:?}", key2.as_ref());
        assert_ne!(key1.as_ref(), key2.as_ref(), "DagknightKey DB bytes must encode k uniquely");

        // Also assert that the differing two-byte `k` slot differs (sanity check on layout)
        let hash_size = kaspa_hashes::HASH_SIZE;
        // k is encoded as two bytes after root_hash
        assert_ne!(
            &key1.as_ref()[hash_size..hash_size + 2],
            &key2.as_ref()[hash_size..hash_size + 2],
            "k slot (two bytes) must differ for different k values"
        );
    }

    #[test]
    fn test_db_dagknight_store_isolates_by_k() {
        use crate::model::stores::dagknight::DbDagknightStore;
        use crate::model::stores::ghostdag::GhostdagData;
        use kaspa_database::prelude::CachePolicy;
        use kaspa_database::prelude::ConnBuilder;
        use kaspa_hashes::Hash;
        use std::sync::Arc;

        // Create a temporary RocksDB
        let (_lifetime, db) = kaspa_database::create_temp_db!(ConnBuilder::default().with_files_limit(10));

        let store = DbDagknightStore::new(db.clone(), CachePolicy::Count(16));

        let root: Hash = 0xAA_u64.into();
        let pov: Hash = 0xBB_u64.into();

        let k1 = 0x0001;
        let k2 = 0x0101; // 2nd byte is the same as above

        // Create two distinct GhostdagData values
        let gd1 = GhostdagData::new(
            10,
            Default::default(),
            Hash::from_u64_word(1),
            Default::default(),
            Default::default(),
            Default::default(),
        );
        let gd2 = GhostdagData::new(
            20,
            Default::default(),
            Hash::from_u64_word(2),
            Default::default(),
            Default::default(),
            Default::default(),
        );

        let key1 = DagknightKey::new(root, pov, k1, false);
        let key2 = DagknightKey::new(root, pov, k2, false);

        // Insert both into the DB-backed store
        store.insert(key1.clone(), Arc::new(gd1)).expect("insert k1");
        store.insert(key2.clone(), Arc::new(gd2)).expect("insert k2");

        // Read them back and verify isolation
        let read1 = store.get_data(key1).expect("read k1");
        let read2 = store.get_data(key2).expect("read k2");

        assert_eq!(read1.blue_score, 10);
        assert_eq!(read2.blue_score, 20);
    }
}
